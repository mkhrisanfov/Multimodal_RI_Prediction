# Gas Chromatographic Retention Index Prediction Using Multimodal Machine Learning : unofficial Python implementation

This repository contains prediction model from the original paper ["Gas Chromatographic Retention Index Prediction Using Multimodal Machine Learning"](https://doi.org/10.1109/ACCESS.2020.3045047) by D.D.Matyushin and A.K.Buryak ported to Python with some adjustments. 
The following libraries are used:

- CatBoost - gradient boosting framework, alternative to XGBoost
- Mordred - generating molecular descriptors
- NumPy - mathematical operations, managing preprocessed data
- Pandas - reading input .ri files, exporting .csv files, dataframe operations
- PyTorch - deep learning framework
- RdKit - reading SMILES, treating molecules as python objects, ECFP4 fingerprints generation
- Scikit-learn - train/test splitting
- tqdm - progress bars
- XGBoost - gradient boosting framework used in the original paper

## CNN1D:

All SMILES during preprocessing are canonicalized (RdKit) and all stereoisomeric information is omitted. This conversion helps to streamline the training process because not all molecular formulas have this information. Due to these changes the number of channels in the first layer becomes **33 instead of 36**.

During training  these changes did not produce any significant effects on prediction accuracy. I was able to achieve MAE of 30.8 i.u. which is slightly less than 31.5 i.u. in the original paper.

## CNN2D:

All SMILES are treated the same way as before. 2D coordinates are generated by RdKit. Sadly, their size is limited to **65x65 instead of 130x130** by the amount of RAM available. It also seems that RdKit outputs less information than CDK and that reduces number of channels in the first layer of the network from **29 to 15**.

These changes shurely made prediction accuracy worse. During training I was not able to reproduce the results in the original paper.

## MLP:

In the orignial paper **243** CDK descriptors are used along with **84** SMARTS queries. In this implementation **1604** Mordred descriptors (all 3D and some topological descriptors that were too slow to calculate were excluded) and **167** MACCS keys in form of SMARTS queries from RdKit are used.

During training these changes could have produced slight improvements in terms of prediction accuracy. I was able to achieve MAE of 30.4 i.u. instead of 30.8 i.u. in the original paper.

## Gradient Boosting:

XGBoost and CatBoost were used as gradient boosting frameworks. Molecular descriptors and MACCS were used as input data.

I was able to improve prediction accuracy to 38.2 i.u. when using XGBoost and to 33.8 i.u. when using CatBoost. These improvements are most likely caused by using more descriptors and SMARTS queries.

## Pre-trained models and training notebooks:

Pre-trained models and the notebooks for training need a bit more work, so they are coming later. Hyperparameters need to be tweaked some more to achieve optimal prediction accuracy.




